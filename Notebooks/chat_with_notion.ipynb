{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Agent with Pinecone & LLaMa for Notion\n",
    "\n",
    "The goal of this notebook is to show how we can use a local version of LLaMa to run RAG on a Pinecone index.\n",
    "In this particular example, we are embedding a Notion Workspace and allowing to QA on the content of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the .env parameters with:\n",
    "- Pinecone API key\n",
    "- Pinecone Environment\n",
    "- OpenAI API key\n",
    "- LLaMa model path\n",
    "- Notion Download Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pinecone-client tqdm langchain llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide if you want to use LLaMa or OpenAI for the embedding\n",
    "embedding_model = \"openai\" # \"openai\" \"llama\"\n",
    "\n",
    "# Decide if you want to use LLaMa or OpenAI for the chat\n",
    "chat_model = \"llama\" # \"openai\" \"llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "# find your environment next to the api key in pinecone console\n",
    "pinecone_env = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "# get our OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "LLaMa_model_path=\"../../../llama/llama-2-7b-chat/ggml-model-f16_q4_0.gguf\"\n",
    "NOTION_DIRECTORY_PATH=\"../notion_data/Support_runbook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "\n",
    "loader = NotionDirectoryLoader(NOTION_DIRECTORY_PATH)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of chunks\n",
    "num_chunks = 0\n",
    "for doc in all_splits:\n",
    "    num_chunks += 1\n",
    "\n",
    "print(f\"Total number of chunks: {num_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "if embedding_model == \"llama\":\n",
    "    embeddings = LlamaCppEmbeddings(model_path=LLaMa_model_path)\n",
    "elif embedding_model == \"openai\":\n",
    "    embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n",
    "pinecone.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# set dimension to 4096 for LLaMa 7B model and 1536 for OpenAI\n",
    "dimension = 4096 if embedding_model == \"llama\" else 1536\n",
    "\n",
    "index_name = 'notion-db-chatbot'\n",
    "\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "\n",
    "# we create a new index\n",
    "pinecone.create_index(\n",
    "    name=index_name,\n",
    "    metric='dotproduct',\n",
    "    dimension=dimension\n",
    ")\n",
    "\n",
    "# wait for index to be initialized\n",
    "while not pinecone.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
    "\n",
    "Now we upsert the data to Pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "vectordb = Pinecone.from_documents(all_splits[:1], embeddings, index_name=index_name)\n",
    "\n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(1, len(all_splits))):\n",
    "    vectordb.add_documents(all_splits[:i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick note on performance.\n",
    "Right now it takes ~20min to embed 300 splits with OpenAI and 10min per split with LLaMa embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've indexed everything, now we can check the number of vectors in our index like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it worked\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a chat agent using Pinecone and our LLaMa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Initialize Llama2 Model\n",
    "llm = LlamaCpp(\n",
    "    model_path=LLaMa_model_path\n",
    "    , temperature=0.0\n",
    "    , max_tokens=2000\n",
    "    , n_gqa=8 # Number of GPUs to use\n",
    "    , top_p=1\n",
    "    , verbose=True # Verbose is required to pass to the callback manager\n",
    "    , f16_kv=True  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    , n_ctx=2048 # context window size\n",
    "    , n_batch=512 # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "    , n_gpu_layers=1 # Metal set to 1 is enough.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Set up the Conversational Retrieval Chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectordb.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a chat agent using Pinecone and our LLaMa model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "chat_history = []\n",
    "while True:\n",
    "    query = input('Prompt: ')\n",
    "    if query.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print('Exiting')\n",
    "        sys.exit()\n",
    "    result = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "    print('Answer: ' + result['answer'] + '\\n')\n",
    "    chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a chat agent using Pinecone and OpenAI GPT3.5 turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initiate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the conversational agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the query from a user input\n",
    "query = input('Prompt: ')\n",
    "\n",
    "agent(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some learnings from this exercise:\n",
    "- OpenAI is much faster than LLaMa (more than 10x)\n",
    "- OpenAI is fairly cheap (embedding 4Mb of Notion data cost us ~$0.36, running GPT3.5 turbo is \"virtually\" free)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
